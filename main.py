#AdaBoost,

#How does it work
#Initially, Adaboost selects a training subset randomly.



#AdaBoost should meet two conditions:

#The classifier should be trained interactively on various weighed training examples.

#In each iteration, it tries to provide an excellent fit for these examples by minimizing training error.


#The advantages are as follows:

#AdaBoost is easy to implement.

#It iteratively corrects the mistakes of the weak classifier and improves accuracy by combining weak learners.

#We can use many base classifiers with AdaBoost.

#AdaBoost is not prone to overfitting.

#The disadvantages are as follows:

#AdaBoost is sensitive to noise data.

#It is highly affected by outliers because it tries to fit each point perfectly.

#AdaBoost is slower compared to XGBoost.